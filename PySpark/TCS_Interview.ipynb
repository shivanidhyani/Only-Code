{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bb0f0a-d83f-406e-bea2-2db09ba679b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea415be-59bd-4c3c-bc13-3c2591f977c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Interview\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a91ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Difference bet truncate, delete, drop\n",
    "DELETE remove the specific row based on the given condition, \n",
    "TRUNCATE removes all the record from the table at once, \n",
    "DROP command removes the table or databases and as well as the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce616b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "How to partition a column \n",
    "In PySpark  Partitioning can be done while writing data to disk or a table, \n",
    "which can significantly improve query performance, especially in distributed processing systems.\n",
    "data_df.write \\\n",
    "    .partitionBy(\"country\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet('output/partitioned_financial_data')\n",
    "\n",
    "Benefits of Partitioning\n",
    "Improved Query Performance: Queries can be more efficient because Spark can skip entire partitions that don’t need to be read.\n",
    "Efficient Data Storage: Partitioning organizes data into smaller, manageable chunks which can be easier to manage and read.\n",
    "Optimized Data Processing: Processing can be distributed across different partitions, leveraging Spark’s parallel processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "one array question on rotate array k times\n",
    "def rotate(nums, k):\n",
    "    n = len(nums)\n",
    "    k = k % n  # To handle the case when k is greater than the array length\n",
    "    \n",
    "    # Helper function to reverse a portion of the array\n",
    "    def reverse(start, end):\n",
    "        while start < end:\n",
    "            nums[start], nums[end] = nums[end], nums[start]\n",
    "            start += 1\n",
    "            end -= 1\n",
    "\n",
    "    # Reverse the entire array\n",
    "    reverse(0, n - 1)\n",
    "    # Reverse the first k elements\n",
    "    reverse(0, k - 1)\n",
    "    # Reverse the remaining elements\n",
    "    reverse(k, n - 1)\n",
    "\n",
    "# Example usage\n",
    "nums1 = [1, 2, 3, 4, 5, 6, 7]\n",
    "k1 = 3\n",
    "rotate(nums1, k1)\n",
    "print(nums1)  # Output: [5, 6, 7, 1, 2, 3, 4]\n",
    "\n",
    "nums2 = [-1, -100, 3, 99]\n",
    "k2 = 2\n",
    "rotate(nums2, k2)\n",
    "print(nums2)  # Output: [3, 99, -1, -100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721dfa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Atomicity: The intire trancation take place at once or doesn't happen at all\n",
    "Consistency: the database must be consistent state both before and after the transaction. \n",
    "Isolation: Multiple transactions can execute concurrently without interfering with each other. \n",
    "Durability: The transaction’s changes are saved to the database permanently, and even if the system crashes, the changes remain intact and can be recovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cfa326",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is the difference between pass and continue in Python?\n",
    "pass and continue are two different statements used for different purposes within loops and control structures.\n",
    "# Placeholder in a loop\n",
    "for i in range(10):\n",
    "    if i % 2 == 0:\n",
    "        pass  # To be implemented later\n",
    "    else:\n",
    "        print(i)\n",
    "\n",
    "# Skipping even numbers\n",
    "for i in range(10):\n",
    "    if i % 2 == 0:\n",
    "        continue  # Skip even numbers\n",
    "    print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e039ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "What is PEP ?\n",
    "Python Enhancement Proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4621f510",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reading a .CSV file using pandas\n",
    "df = pd.read_csv('path/to/your/file.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267b4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lazy Evaluation: Spark transformations done using Spark RDDs are lazy. Meaning, \n",
    "they do not generate results right away, but they create new RDDs from existing RDD. \n",
    "This lazy evaluation increases the system efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9328e016",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fault Tolerance: Spark supports fault tolerance using RDD. \n",
    "Spark RDDs are the abstractions designed to handle failures of worker nodes which ensures zero data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5858571f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Company = StructType([\n",
    "    StructField(\"dates\", StringType(), True),\n",
    "    StructField(\"college\", StringType(), True),\n",
    "    StructField(\"company\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"package\", FloatType(), True),\n",
    "    StructField(\"selected\", IntegerType(), True),\n",
    "    StructField(\"participated\", IntegerType(), True),\n",
    "    StructField(\"criteria\", IntegerType(), True)\n",
    "])\n",
    "companyRDD = spark.sparkContext.textFile(\"Company.txt\")\n",
    "companyDF = spark.createDataFrame(companyRDD.map(lambda x: x.split(\"|\")), schema=Company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683bc050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
