{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2bb0f0a-d83f-406e-bea2-2db09ba679b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea415be-59bd-4c3c-bc13-3c2591f977c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/24 15:43:28 WARN Utils: Your hostname, Shivanis-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.106 instead (on interface en0)\n",
      "24/04/24 15:43:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/24 15:43:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Interview\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "068e9fb3-6ac2-4e6f-abdb-40c9250dfcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pyspark/python/lib/pyspark.zip/pyspark/daemon.py:154: DeprecationWarning: This process (pid=31973) is multi-threaded, use of fork() may lead to deadlocks in the child.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "| Name|          Hobbies|\n",
      "+-----+-----------------+\n",
      "|Alice|Badminton, Tennis|\n",
      "|  Bob|  Tennis, Circket|\n",
      "|Julie| Cricket, Carroms|\n",
      "+-----+-----------------+\n",
      "\n",
      "+-----+---------+\n",
      "| Name|  Hobbies|\n",
      "+-----+---------+\n",
      "|Alice|Badminton|\n",
      "|Alice|   Tennis|\n",
      "|  Bob|   Tennis|\n",
      "|  Bob|  Circket|\n",
      "|Julie|  Cricket|\n",
      "|Julie|  Carroms|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Question 1. Write a PySpark Query using below input to get below output.  #KPMG\n",
    "Input \n",
    "+-----+---------------------+\n",
    "|  Name|             Hobbies|\n",
    "+-----+---------------------+\n",
    "| Alice|  Badmintion, Tennis|\n",
    "|   Bob|     Tennis, Cricket|\n",
    "| Julie|    Cricket, Carroms|\n",
    "+-----+---------------------+\n",
    "Output\n",
    "+-----+------------+\n",
    "|  Name|    Hobbies|\n",
    "+-----+------------+\n",
    "| Alice| Badmintion|\n",
    "| Alice|     Tennis|\n",
    "|   Bob|     Tennis|\n",
    "|   Bob|    Cricket|\n",
    "| Julie|    Cricket|\n",
    "| Julie|    Carroms|\n",
    "+-----+------------+\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "data = [('Alice', 'Badminton, Tennis' ), ('Bob',  'Tennis, Circket'), ('Julie', 'Cricket, Carroms')]\n",
    "columns = [\"Name\", \"Hobbies\"]\n",
    "df1 = spark.createDataFrame(data, columns)\n",
    "df1.show()\n",
    "\n",
    "#split : In PySpark, the split() function is used to split a string column into an array of substrings based on a delimiter.\n",
    "#explode : function is used to split a column that contains an array or map type into multiple rows, with each element of the array or map occupying its own row. \n",
    "from pyspark.sql.functions import split, explode\n",
    "df1= df1.select(df1.Name, explode(split(df1.Hobbies, ',')).alias(\"Hobbies\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd7fda3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|City1|City2|City3|\n",
      "+-----+-----+-----+\n",
      "|  Goa|     |   AP|\n",
      "|     |   AP| NULL|\n",
      "| NULL|     | Bglu|\n",
      "+-----+-----+-----+\n",
      "\n",
      "+------+\n",
      "|Result|\n",
      "+------+\n",
      "|   Goa|\n",
      "|    AP|\n",
      "|  Bglu|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Question2. Write a PySpark Query using below input to get below output.  #KPMG\n",
    "Input\n",
    "+-------+-------+-------+\n",
    "| City 1| City 2| City 3|\n",
    "+-------+-------+-------+\n",
    "|    Goa|       |     AP|\n",
    "|       |     AP|   null|\n",
    "|   null|       |   Bglu|\n",
    "+-------+-------+-------+\n",
    "\n",
    "Output\n",
    "+-------+\n",
    "| Result| \n",
    "+-------+\n",
    "|    Goa|\n",
    "|     AP| \n",
    "|   Bglu|\n",
    "+-------+\n",
    "\n",
    "'''\n",
    "\n",
    "df2 = data = [('Goa', '', 'AP' ), ('',  'AP', None), (None, '', 'Bglu')]\n",
    "columns = [\"City1\", \"City2\", \"City3\"]\n",
    "df2 = spark.createDataFrame(data, columns)\n",
    "df2.show()\n",
    "\n",
    "\n",
    "#coalesce : In PySpark, the coalesce() function is used to return the first non-null value among the input columns. \n",
    "#It takes multiple columns as input and returns a new column with the first non-null value from those columns.\n",
    "from pyspark.sql.functions import coalesce,when\n",
    "df2 = df2.withColumn('Result', coalesce(\n",
    "    when(df2.City1 == \"\", None).otherwise(df2.City1), \n",
    "    when(df2.City2 == \"\", None).otherwise(df2.City2), \n",
    "    when(df2.City3 == \"\", None).otherwise(df2.City3)\n",
    "    )).drop(df2.City1, df2.City2, df2.City3)\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6465118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| Id| Name|\n",
      "+---+-----+\n",
      "|  1|Steve|\n",
      "|  2|David|\n",
      "|  3| John|\n",
      "|  4|Shree|\n",
      "|  5|Helen|\n",
      "+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+----------+------------+\n",
      "| Id| Name|Percentage|      Result|\n",
      "+---+-----+----------+------------+\n",
      "|  1|Steve|      95.0|  Distintion|\n",
      "|  2|David|      65.0| First Class|\n",
      "|  3| John|      25.0|        Fail|\n",
      "|  4|Shree|      50.0|Second Class|\n",
      "|  5|Helen|      45.0| Third Class|\n",
      "+---+-----+----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''Question3. You have 2 Dataframe as Input and you have to get the below Output \n",
    "Input \n",
    "+---+-----+         +---+-------+----+\n",
    "| Id| Name|         | Id|Subject|Mark|\n",
    "+---+-----+         +---+-------+----+\n",
    "|  1|Steve|         |  1|    SQL|  90|\n",
    "|  2|David|         |  1|PySpark| 100|\n",
    "|  3| John|         |  2|    SQL|  70|\n",
    "|  4|Shree|         |  2|PySpark|  60|\n",
    "|  5|Helen|         |  3|    SQL|  30|\n",
    "+---+-----+         |  3|PySpark|  20|\n",
    "                    |  4|    SQL|  50|\n",
    "                    |  4|PySpark|  50|\n",
    "                    |  5|    SQL|  45|\n",
    "                    |  5|PySpark|  45|\n",
    "                    +---+-------+----+\n",
    "\n",
    "Output\n",
    "+---+-----+----------+------------+\n",
    "| Id| Name|Percentage|      Result|\n",
    "+---+-----+----------+------------+\n",
    "|  1|Steve|      95.0|  Distintion|\n",
    "|  2|David|      65.0| First Class|\n",
    "|  3| John|      25.0|        Fail|\n",
    "|  4|Shree|      50.0|Second Class|\n",
    "|  5|Helen|      45.0| Third Class|\n",
    "+---+-----+----------+------------+                    \n",
    "\n",
    "'''\n",
    "\n",
    "data1=[(1,\"Steve\"),(2,\"David\"),(3,\"John\"),(4,\"Shree\"),(5,\"Helen\")]\n",
    "data2=[(1,\"SQL\",90),(1,\"PySpark\",100),(2,\"SQL\",70),(2,\"PySpark\",60),(3,\"SQL\",30),(3,\"PySpark\",20),(4,\"SQL\",50),(4,\"PySpark\",50),(5,\"SQL\",45),(5,\"PySpark\",45)]\n",
    "schema1=[\"Id\",\"Name\"]\n",
    "schema2=[\"Id\",\"Subject\",\"Mark\"]\n",
    "\n",
    "student_df = spark.createDataFrame(data1,schema1)\n",
    "student_df.show()\n",
    "marks_df = spark.createDataFrame(data2,schema2)\n",
    "joined_df = student_df.join(marks_df, student_df.Id == marks_df.Id).drop(marks_df.Id)\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "joined_df = joined_df.groupBy('Id', 'Name').agg((sum('Mark')/count('*')).alias(\"Percentage\"))\n",
    "\n",
    "result_df = joined_df.select('*', \n",
    "                             (when(joined_df.Percentage >= 70, 'Distintion')\n",
    "                             .when((joined_df.Percentage < 70) &  (joined_df.Percentage >=60), 'First Class')\n",
    "                             .when((joined_df.Percentage < 60) &  (joined_df.Percentage >=50), 'Second Class')\n",
    "                             .when((joined_df.Percentage < 50) & (joined_df.Percentage >= 40), 'Third Class')\n",
    "                             .when((joined_df.Percentage <=39), 'Fail')).alias(\"Result\")\n",
    "                             )\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96e6c63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+--------+\n",
      "|EmpId|EmpName|Salary|DeptName|\n",
      "+-----+-------+------+--------+\n",
      "|    4|    Den|  3000|      HR|\n",
      "|    3|    Col|  2500|      IT|\n",
      "|    7|   Goer|  4000|   Sales|\n",
      "|    8|    Hen|  4000|   Sales|\n",
      "+-----+-------+------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "'''Question4. Employee Who are getting Higest salary in their department\n",
    "Input\n",
    "+-----+-------+------+--------+\n",
    "|EmpId|EmpName|Salary|DeptName|\n",
    "+-----+-------+------+--------+\n",
    "|    1|   Aman|  1000|      IT|\n",
    "|    2|    Bob|  1500|      IT|\n",
    "|    3|    Col|  2500|      IT|\n",
    "|    4|    Den|  3000|      HR|\n",
    "|    5|  Eimly|  2000|      HR|\n",
    "|    6|    Fan|  1000|      HR|\n",
    "|    7|   Goer|  4000|   Sales|\n",
    "|    8|    Hen|  4000|   Sales|\n",
    "|    9|   Iton|  1000|   Sales|\n",
    "|   10|   Jack|  2000|   Sales|\n",
    "+-----+-------+------+--------+\n",
    "\n",
    "Output \n",
    "-----+-------+------+--------+\n",
    "|EmpId|EmpName|Salary|DeptName|\n",
    "+-----+-------+------+--------+\n",
    "|    3|    Col|  2500|      IT|\n",
    "|    4|    Den|  3000|      HR|\n",
    "|    7|   Goer|  4000|   Sales|\n",
    "|    8|    Hen|  4000|   Sales|\n",
    "+-----+-------+------+--------+\n",
    "\n",
    "'''\n",
    "data=[(1,\"Aman\",1000,\"IT\"),(2,\"Bob\",1500,\"IT\"),(3,\"Col\",2500,\"IT\"),(4,\"Den\",3000,\"HR\"),(5,\"Eimly\",2000,\"HR\"),(6,\"Fan\",1000,\"HR\")\n",
    "       ,(7,\"Goer\",4000,\"Sales\"),(8,\"Hen\",4000,\"Sales\"),(9,\"Iton\",1000,\"Sales\"),(10,\"Jack\",2000,\"Sales\")]\n",
    "schema=[\"EmpId\",\"EmpName\",\"Salary\",\"DeptName\"]\n",
    "df=spark.createDataFrame(data,schema)\n",
    "\n",
    "# df = df.groupby('DeptName').agg(max('Salary').alias('Salary'))\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "\n",
    "df= df.select('*', dense_rank().over(Window.partitionBy(df.DeptName).orderBy(df.Salary.desc())).alias(\"Rank\"))\n",
    "df = df.filter(df.Rank==1).drop(df.Rank)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5a1a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+-------+----------+-----------+-----------+\n",
      "|DeptName|MgrName|EmpName|SalaryYear|SalaryMonth|sum(Salary)|\n",
      "+--------+-------+-------+----------+-----------+-----------+\n",
      "|      IT|    Raj| Joanne|      2023|    January|      12520|\n",
      "|      IT|   NULL|    Raj|      2023|    January|      50000|\n",
      "+--------+-------+-------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Question 5.  Given 2 Input DataFram find out the \n",
    "1. Create a Dataframe with these columns with the given Input :-> DeptName, MgrName, EmpName, SalaryYear, SalaryMonth, TotalMonthlySalary\n",
    "2. 7th Higest Salaries and Show EmpName\n",
    "#KPMG\n",
    "\n",
    "Input DataFrames\n",
    "df_salary                                                   df_dept\n",
    "+-----+-------+-----+------+----------+------+              +------+--------+           \n",
    "|EmpId|EmpName|MgrId|DeptId|SalaryDate|Salary|              |DeptId|DeptName|\n",
    "+-----+-------+-----+------+----------+------+              +------+--------+\n",
    "|  100|    Raj| NULL|     1|  01-04-23| 50000|              |     1|      IT|\n",
    "|  200| Joanne|  100|     1|  01-04-23|  4000|              |     2|      HR|\n",
    "|  200| Joanne|  100|     1|  13-04-23|  4500|              +------+--------+\n",
    "|  200| Joanne|  100|     1|  14-04-23|  4020|\n",
    "+-----+-------+-----+------+----------+------+    \n",
    "'''\n",
    "\n",
    "data1=[(100,\"Raj\",None,1,\"01-04-23\",50000), (200,\"Joanne\",100,1,\"01-04-23\",4000),(200,\"Joanne\",100,1,\"13-04-23\",4500),(200,\"Joanne\",100,1,\"14-04-23\",4020)]\n",
    "schema1=[\"EmpId\",\"EmpName\",\"MgrId\",\"DeptId\",\"SalaryDate\",\"Salary\"]\n",
    "df_salary=spark.createDataFrame(data1,schema1)\n",
    "data2=[(1,\"IT\"),(2,\"HR\")]\n",
    "schema2=[\"DeptId\",\"DeptName\"]\n",
    "df_dept=spark.createDataFrame(data2,schema2)\n",
    "\n",
    "df = df_salary.withColumn(\"NewSalaryDate\", to_date('SalaryDate', 'dd-mm-yy')).drop(df_salary.SalaryDate)\n",
    "df = df.join(df_dept, df_dept.DeptId == df.DeptId).drop(df_dept.DeptId )\n",
    "\n",
    "df = df.alias('a').join(df.alias('b'), col('a.MgrId') == col('b.EmpId'), 'left').select(\n",
    "    col('a.DeptName'), \n",
    "    col('b.EmpName').alias('MgrName'),\n",
    "    col('a.EmpName'),\n",
    "    col('a.NewSalaryDate'),\n",
    "    col('a.Salary')\n",
    ")\n",
    "\n",
    "df = df.groupBy('DeptName', 'MgrName', 'EmpName', year('NewSalaryDate').alias('SalaryYear'), date_format('NewSalaryDate', 'MMMM').alias('SalaryMonth')).sum('Salary')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffee5456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n",
      "| ID|     movie|description|rating|\n",
      "+---+----------+-----------+------+\n",
      "|  5|House card|Interesting|   9.1|\n",
      "|  1|       War|   great 3D|   8.9|\n",
      "+---+----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Question 6.\n",
    "Write a Pyspark query to report the movies with an odd-numbered ID and a description that is not \"boring\".\n",
    "Return the result table in descending order by rating.\n",
    "Input \n",
    "+---+----------+-----------+------+\n",
    "| ID|     movie|description|rating|\n",
    "+---+----------+-----------+------+\n",
    "|  1|       War|   great 3D|   8.9|\n",
    "|  2|   Science|    fiction|   8.5|\n",
    "|  3|     irish|     boring|   6.2|\n",
    "|  4|  Ice song|    Fantacy|   8.6|\n",
    "|  5|House card|Interesting|   9.1|\n",
    "+---+----------+-----------+------+\n",
    "\n",
    "Output\n",
    "+---+----------+-----------+------+\n",
    "| ID|     movie|description|rating|\n",
    "+---+----------+-----------+------+\n",
    "|  5|House card|Interesting|   9.1|\n",
    "|  1|       War|   great 3D|   8.9|\n",
    "+---+----------+-----------+------+\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "data=[(1, 'War','great 3D',8.9),\n",
    "      (2, 'Science','fiction',8.5),\n",
    "      (3, 'irish','boring',6.2),\n",
    "      (4, 'Ice song','Fantacy',8.6),\n",
    "      (5, 'House card','Interesting',9.1)\n",
    "      ]    \n",
    "schema=\"ID int,movie string,description string,rating double\"\n",
    "df=spark.createDataFrame(data,schema)\n",
    "df=df.select('*').filter((col('ID')%2!=0) & (col('description')!='boring')).orderBy(col('rating').desc())\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0103f3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
